# -*- coding: utf-8 -*-
"""Copy of BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GA_zxNPCps4Ulkau8uzfE0bHSBvQMFGT
"""



import os
import re

try:
#     %tensorflow_version 2.x
except Exception:
    pass
import tensorflow as tf

from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import torch

import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
from textwrap import wrap

from torch import nn, optim
from torch.utils.data import Dataset, DataLoader

%matplotlib inline
%config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]
sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
rcParams['figure.figsize'] = 12, 8


from google.colab import drive
drive.flush_and_unmount()
drive.mount('/content/drive')

path = "/content/drive/My Drive"

"""# BERT TOKENIZER"""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

vocabulary = tokenizer.get_vocab()

print(list(vocabulary.keys())[12528])

"""
# MODEL
"""

class NewsDataset(Dataset):

  def __init__(self, all_content, labels, tokenizer, max_len):
    self.all_content = all_content
    self.labels = labels
    self.tokenizer = tokenizer
    self.max_len = max_len

  def __len__(self):
    return len(self.all_content)

  def __getitem__(self, item):
    content = (str(self.all_content[item])).lower()
    label = self.labels[item]

    encoding = self.tokenizer.encode_plus(
      content,
      add_special_tokens=True,
      max_length=self.max_len,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt'
    )

    return {
      'content_text': content,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'labels': torch.tensor(label, dtype=torch.long)
    }

def create_data_loader(df, tokenizer, max_len, batch_size):
  ds = NewsDataset(
    all_content=df['content'].to_numpy(),
    labels=df['label'].to_numpy(),
    tokenizer=tokenizer,
    max_len=max_len
  )

  return DataLoader(
    ds,
    batch_size=batch_size,
    num_workers=4  
  )

bert_model = BertModel.from_pretrained('bert-base-uncased')

class FakeDataClassifier(nn.Module):

    def __init__(self, n_classes):
        super(FakeDataClassifier, self).__init__()
        self.bert = bert_model
        self.drop = nn.Dropout(0.3)
        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
            
    
    def forward(self, input_ids, attention_mask):
        _, pooled_output = self.bert(
          input_ids=input_ids,
          attention_mask=attention_mask
        )
        output = self.drop(pooled_output)
        return self.out(output)
    def unfreeze(self,start_layer,end_layer):
        def children(m):
            return m if isinstance(m, (list, tuple)) else list(m.children())
        def set_trainable_attr(m, b):
            m.trainable = b
            for p in m.parameters():
                p.requires_grad = b
        def apply_leaf(m, f):
            c = children(m)
            if isinstance(m, nn.Module):
                f(m)
            if len(c) > 0:
                for l in c:
                    apply_leaf(l, f)
        def set_trainable(l, b):
            apply_leaf(l, lambda m: set_trainable_attr(m, b))

        # You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)
        set_trainable(self.bert, False)
        for i in range(start_layer, end_layer+1):
            set_trainable(self.bert.encoder.layer[i], True)

def train_epoch(
  model,
  data_loader,
  loss_fn,
  optimizer,
  device,
  scheduler,
  n_examples
):
  print("Training model")
  model = model.train()

  losses = []
  correct_predictions = 0
  
  print("Ilterating on data")
  print(len(data_loader))
  i = 0
  for d in data_loader:
    input_ids = d["input_ids"].to(device)
    attention_mask = d["attention_mask"].to(device)
    labels = d["labels"].to(device)

    outputs = model(
      input_ids=input_ids,
      attention_mask=attention_mask
    )

    _, preds = torch.max(outputs, dim=1)
    loss = loss_fn(outputs, labels)
     

    correct_predictions += torch.sum(preds == labels)
    losses.append(loss.item())

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()
  return correct_predictions.double() / n_examples, np.mean(losses)

def eval_model(model, data_loader, loss_fn, device, n_examples):
  model = model.eval()

  losses = []
  correct_predictions = 0

  with torch.no_grad():
    for d in data_loader:
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      labels = d["labels"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      )
      _, preds = torch.max(outputs, dim=1)

      loss = loss_fn(outputs, labels)

      correct_predictions += torch.sum(preds == labels)
      losses.append(loss.item())

  return correct_predictions.double() / n_examples, np.mean(losses)

def get_predictions(model, data_loader):
  model = model.eval()

  content_texts = []
  predictions = []
  prediction_probs = []
  real_values = []

  with torch.no_grad():
    for d in data_loader:
      texts = d["content_text"]
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      labels = d["labels"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      )
      _, preds = torch.max(outputs, dim=1)
      content_texts.extend(texts)
      predictions.extend(preds)
      prediction_probs.extend(outputs)
      real_values.extend(labels)

  predictions = torch.stack(predictions).cpu()
  prediction_probs = torch.stack(prediction_probs).cpu()
  real_values = torch.stack(real_values).cpu()
  return content_texts, predictions, prediction_probs, real_values

"""
# Gossip Data
"""

gossip_data = pd.read_csv(os.path.join(path,'OSNA-FSL/gossipcop_content_no_ignore.tsv'), sep='\t')
gossip_data.head()

gossip_data.shape

RANDOM_SEED = 60616
train, test = train_test_split(gossip_data, test_size=0.1, random_state=RANDOM_SEED, shuffle=True)
val, test = train_test_split(
  test,
  test_size=0.5,
  random_state=RANDOM_SEED
)

BATCH_SIZE = 8
MAX_LEN = 512

train_data_loader = create_data_loader(train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(test, tokenizer, MAX_LEN, BATCH_SIZE)

data = next(iter(train_data_loader))
data.keys()

class_names = ['fake', 'true']
model = FakeDataClassifier(2)
model = model.to(device)

input_ids = data['input_ids'].to(device)
attention_mask = data['attention_mask'].to(device)

print(input_ids.shape) # batch size x seq length
print(attention_mask.shape) # batch size x seq length
torch.Size([16, 160])
torch.Size([16, 160])

input_ids

EPOCHS = 3

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps_g = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps_g
)

loss_fn = nn.CrossEntropyLoss().to(device)

"""## Training the model"""

%%time

history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,
    loss_fn,
    optimizer,
    device,
    scheduler,
    len(train)
  )

  print(f'Train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn,
    device,
    len(val)
  )

  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc > best_accuracy:
    torch.save(model.state_dict(), 'best_model_state.bin')
    best_accuracy = val_acc

plt.plot(history['train_acc'], label='train accuracy')
plt.plot(history['val_acc'], label='validation accuracy')

plt.title('Training history')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1]);

test_acc, _ = eval_model(
  model,
  test_data_loader,
  loss_fn,
  device,
  len(test)
)

test_acc.item()

y_content, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  test_data_loader
)

print(classification_report(y_test, y_pred, target_names=class_names))

def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True sentiment')
  plt.xlabel('Predicted sentiment');

cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
show_confusion_matrix(df_cm)

"""
# Half Labeled Data
"""

train_fact = train.loc[train['label'] == 1]
train_fake = train.loc[train['label'] == 0]

fifty_percent = int(train.shape[0] / 4)
ten_percent = int(train.shape[0] * 0.1)


train_fact_25_ids = np.array(train_fact[0: fifty_percent]['id'])
train_fake_25_ids = np.array(train_fake[0: fifty_percent]['id'])

train_50_labelled_ids = np.concatenate([train_fact_25_ids, train_fake_25_ids])

train_new = train
train_50_labeled = pd.DataFrame(columns=['id', 'label', 'content'])
train_50_unlabeled = pd.DataFrame(columns=['id', 'label', 'content'])

rows = train.shape[0]
for idx, row in train_new.iterrows():
  id = train_new.loc[idx, 'id']
  label = train_new.loc[idx, 'label']
  if id not in train_50_labelled_ids:
    train_50_unlabeled = train_50_unlabeled.append({'id': id, 'label': 0, 'content': train_new.loc[idx, 'content'] }, 
                                                   ignore_index=True)
  elif id in train_50_labelled_ids:
    train_50_labeled = train_50_labeled.append({'id': id, 'label': train_new.loc[idx, 'label'], 'content': train_new.loc[idx, 'content'] }, 
                                                   ignore_index=True)

train_50_labeled.to_csv(os.path.join(path,'OSNA-FSL/train_50_labeled.csv'))

train_50, val_50 = train_test_split(train_50_labeled, test_size = 0.1, random_state=60616)

"""
### Train on 50% labeled
"""

BATCH_SIZE = 8
MAX_LEN = 512

train_data_loader = create_data_loader(train_50, tokenizer, MAX_LEN, BATCH_SIZE)
train_50_unlabeled_data_loader = create_data_loader(train_50_unlabeled, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(val_50, tokenizer, MAX_LEN, BATCH_SIZE)

class_names = ['fake', 'true']
model = FakeDataClassifier(len(class_names))
model = model.to(device)

data = next(iter(train_data_loader))

input_ids = data['input_ids'].to(device)
attention_mask = data['attention_mask'].to(device)

EPOCHS = 5

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps_g = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps_g
)

loss_fn = nn.CrossEntropyLoss().to(device)

## Train the model
%%time

history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,
    loss_fn,
    optimizer,
    device,
    scheduler,
    len(train_50)
  )

  print(f'Train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn,
    device,
    len(val_50)
  )

  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc > best_accuracy:
    torch.save(model.state_dict(), 'best_model_state.bin')
    best_accuracy = val_acc

y_content, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  train_50_unlabeled_data_loader
)

predicted_data = pd.DataFrame() 
predicted_data['content'] = y_content
predicted_data['label'] = y_pred

trained_data_50_orig = train_50_labeled[['label', 'content']]
trained_data_50 = pd.concat([trained_data_50_orig, predicted_data])


BATCH_SIZE = 8
MAX_LEN = 512

train_data_loader = create_data_loader(trained_data_50, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(test, tokenizer, MAX_LEN, BATCH_SIZE)

model = FakeDataClassifier(len(class_names))
model = model.to(device)

data = next(iter(train_data_loader))

input_ids = data['input_ids'].to(device)
attention_mask = data['attention_mask'].to(device)


EPOCHS = 10

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps_g = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps_g
)

loss_fn = nn.CrossEntropyLoss().to(device)

## Training the model
%%time

history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,
    loss_fn,
    optimizer,
    device,
    scheduler,
    len(trained_data_50)
  )

  print(f'Train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn,
    device,
    len(val)
  )

  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc > best_accuracy:
    torch.save(model.state_dict(), 'best_model_state.bin')
    best_accuracy = val_acc

plt.plot(history['train_acc'], label='train accuracy')
plt.plot(history['val_acc'], label='validation accuracy')

plt.title('Training history')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1]);

test_acc, _ = eval_model(
  model,
  test_data_loader,
  loss_fn,
  device,
  len(test)
)


y_content, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  test_data_loader
)

print(classification_report(y_test, y_pred, target_names=class_names))

def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True values')
  plt.xlabel('Predicted values');

cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
show_confusion_matrix(df_cm)



"""
# 10% of Data
"""

train_fact = train.loc[train['label'] == 1]
train_fake = train.loc[train['label'] == 0]

five_percent = int(train.shape[0] * 0.05)

train_fact_5_ids = np.array(train_fact[0: five_percent]['id'])
train_fake_5_ids = np.array(train_fake[0: five_percent]['id'])

train_5_labelled_ids = np.concatenate([train_fact_5_ids, train_fake_5_ids])

train_new_10 = train
train_5_labeled = pd.DataFrame(columns=['id', 'label', 'content'])
train_5_unlabeled = pd.DataFrame(columns=['id', 'label', 'content'])

rows = train.shape[0]
for idx, row in train_new_10.iterrows():
  id = train_new_10.loc[idx, 'id']
  label = train_new_10.loc[idx, 'label']
  if id not in train_5_labelled_ids:
    train_5_unlabeled = train_5_unlabeled.append({'id': id, 'label': 0, 'content': train_new_10.loc[idx, 'content'] }, 
                                                   ignore_index=True)
  elif id in train_5_labelled_ids:
    train_5_labeled = train_5_labeled.append({'id': id, 'label': train_new_10.loc[idx, 'label'], 'content': train_new_10.loc[idx, 'content'] }, 
                                                   ignore_index=True)

train_5_labeled.to_csv(os.path.join(path,'OSNA-FSL/train_5_labeled.csv'))

train_10, val_10 = train_test_split(train_5_labeled, test_size = 0.1, random_state=60616)

train_data_loader = create_data_loader(train_10, tokenizer, MAX_LEN, BATCH_SIZE)
train_10_unlabeled_data_loader = create_data_loader(train_5_unlabeled, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(val_10, tokenizer, MAX_LEN, BATCH_SIZE)

model = FakeDataClassifier(len(class_names))
model = model.to(device)

data = next(iter(train_data_loader))
data.keys()

input_ids = data['input_ids'].to(device)
attention_mask = data['attention_mask'].to(device)

EPOCHS = 5

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps_g = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps_g
)

loss_fn = nn.CrossEntropyLoss().to(device)

## Train the model
%%time

history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,
    loss_fn,
    optimizer,
    device,
    scheduler,
    len(train_10)
  )

  print(f'Train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn,
    device,
    len(val_10)
  )

  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc > best_accuracy:
    torch.save(model.state_dict(), 'best_model_state.bin')
    best_accuracy = val_acc

y_content, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  train_10_unlabeled_data_loader
)

predicted_data = pd.DataFrame() 
predicted_data['content'] = y_content
predicted_data['label'] = y_pred

trained_data_10_orig = train_5_labeled[['label', 'content']]
trained_data_10 = pd.concat([trained_data_10_orig, predicted_data])


train_data_loader = create_data_loader(trained_data_10, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(test, tokenizer, MAX_LEN, BATCH_SIZE)

model = FakeDataClassifier(len(class_names))
model = model.to(device)

data = next(iter(train_data_loader))
data.keys()

input_ids = data['input_ids'].to(device)
attention_mask = data['attention_mask'].to(device)


EPOCHS = 10

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps_g = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps_g
)

loss_fn = nn.CrossEntropyLoss().to(device)

## Train the model
%%time

history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,
    loss_fn,
    optimizer,
    device,
    scheduler,
    len(trained_data_10)
  )

  print(f'Train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn,
    device,
    len(val)
  )

  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

  if val_acc > best_accuracy:
    torch.save(model.state_dict(), 'best_model_state.bin')
    best_accuracy = val_acc

plt.plot(history['train_acc'], label='train accuracy')
plt.plot(history['val_acc'], label='validation accuracy')

plt.title('Training history')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1]);

test_acc, _ = eval_model(
  model,
  test_data_loader,
  loss_fn,
  device,
  len(test)
)

test_acc.item()

y_content, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  test_data_loader
)

print(classification_report(y_test, y_pred, target_names=class_names))

def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True values')
  plt.xlabel('Predicted values');

cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)
show_confusion_matrix(df_cm)