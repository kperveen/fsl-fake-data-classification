# -*- coding: utf-8 -*-
"""Copy of reptile-code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jajsEGtR5AvWyCbBWpQB5vVgCZDEr65q
"""

import matplotlib.pyplot as plt
import numpy as np
import random
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_datasets as tfds
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from keras.models import Sequential

path = "/content/drive/My Drive"

from google.colab import drive
drive.flush_and_unmount()
drive.mount('/content/drive')

"""
## Define the Hyperparameters
"""

learning_rate = 0.003
meta_step_size = 0.25

inner_batch_size = 25
eval_batch_size = 25

meta_iters = 4000
eval_iters = 5
inner_iters = 4

eval_interval = 1
train_shots = 20
shots = 5
classes = 2

RANDOM_SEED = 60616

gossip_data = pd.read_csv(os.path.join(path,'OSNA-FSL/gossipcop_content_no_ignore.tsv'), sep='\t')
gossip_data.head()

gossips = gossip_data['content'].values
y = gossip_data['label'].values

X_train_content, X_test_content, y_train, y_test = train_test_split(gossips, y, test_size=0.15, random_state=1000, shuffle = True)

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(min_df=0, lowercase=False)

from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train_content)

X_train = tokenizer.texts_to_sequences(X_train_content)
X_test = tokenizer.texts_to_sequences(X_test_content)

vocab_size = len(tokenizer.word_index) + 1

from keras.preprocessing.sequence import pad_sequences

maxlen = 512
X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

class Dataset:
    def __init__(self, training):

        self.data = {}
        ds = X_train if training else X_train

        for index in range(len(ds)):
            content = X_train[index]
            label = y_train[index]
            if label not in self.data:
                self.data[label] = []
            self.data[label].append(content)
            self.labels = list(self.data.keys())

    def get_mini_dataset(
        self, batch_size, repetitions, shots, num_classes, split=False
    ):
        temp_labels = np.zeros(shape=(5 * shots))
        temp_images = np.zeros(shape=(5 * shots, 512))
        if split:
            test_labels = np.zeros(shape=(5))
            test_images = np.zeros(shape=(5, 512))
        # Get a random subset of labels from the entire label set.
        label_subset = random.choices(self.labels, k=5)

        for class_idx, class_obj in enumerate(label_subset):

            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_obj

            if split:
                test_labels[class_idx] = class_obj

                images_to_split = random.choices(
                    self.data[label_subset[class_idx]], k=shots + 1
                )

                test_images[class_idx] = images_to_split[-1]
                temp_images[
                    class_idx * shots : (class_idx + 1) * shots
                ] = images_to_split[:-1]
            else:
                random_samples = random.choices(self.data[class_obj], k=shots)
                temp_images[ class_idx * shots : (class_idx + 1) * shots] = random_samples

        dataset = tf.data.Dataset.from_tensor_slices(
            (temp_images.astype(np.float32), temp_labels.astype(np.int32))
        )
        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)
        if split:
            return dataset, test_images, test_labels
        return dataset

        

train_dataset = Dataset(training=True)
test_dataset = Dataset(training=False)

"""## 
Build the model
"""

embedding_dim = 512


model = Sequential()
model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))
model.add(layers.Conv1D(128, 5, activation='relu'))
model.add(layers.GlobalMaxPooling1D())
model.add(layers.Dense(120, activation='relu'))
model.add(layers.Dense(2, activation='softmax'))
history = model.compile(metrics= ['acc'])
optimizer = keras.optimizers.SGD(learning_rate=learning_rate)

"""
## Train the model
"""

import tensorflow.keras.backend as keras_backend
training = []
testing = []
y_test_labels = []
y_test_preds = []

starting_point = 0
for meta_iter in range(meta_iters):
    frac_done = meta_iter / meta_iters
    cur_meta_step_size = (1 - frac_done) * meta_step_size
    # Temporarily save the weights from the model.
    old_vars = model.get_weights()
    # Get a sample from the full dataset.
    mini_dataset = train_dataset.get_mini_dataset(
        inner_batch_size, inner_iters, train_shots, classes
    )

    for images, labels in mini_dataset:
        with tf.GradientTape() as tape:
            preds = model(images)
            loss = keras_backend.mean(keras.losses.sparse_categorical_crossentropy(labels, preds))
        grads = tape.gradient(loss, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))
    new_vars = model.get_weights()
    # Perform SGD for the meta step.
    for var in range(len(new_vars)):
        new_vars[var] = old_vars[var] + (
            (new_vars[var] - old_vars[var]) * cur_meta_step_size
        )
    # After the meta-learning step, reload the newly-trained weights into the model.
    model.set_weights(new_vars)
    # Evaluation loop
    if meta_iter % eval_interval == 0:
        accuracies = []
        for dataset in (train_dataset, test_dataset):
            # Sample a mini dataset from the full dataset.
            train_set, test_images, test_labels = dataset.get_mini_dataset(
                eval_batch_size, eval_iters, shots, classes, split=True
            )
            old_vars = model.get_weights()
            # Train on the samples and get the resulting accuracies.
            for images, labels in train_set:
                with tf.GradientTape() as tape:
                    preds = model(images)
                    loss = keras_backend.mean(keras.losses.sparse_categorical_crossentropy(labels, preds))
                grads = tape.gradient(loss, model.trainable_weights)
                optimizer.apply_gradients(zip(grads, model.trainable_weights))
            test_preds = model.predict(test_images)

            test_preds = tf.argmax(test_preds, 1).numpy()
            for l in range(len(test_labels)):
                y_test_labels.append(labels[l])
                y_test_preds.append(test_preds[l])
            num_correct = (test_preds == test_labels).sum()
            accuracies.append(num_correct / len(test_labels))
        training.append(accuracies[0])
        testing.append(accuracies[1])
        if meta_iter % 100 == 0:
            print(
                "batch %d: train=%f test=%f" % (meta_iter, accuracies[0], accuracies[1])
            )

"""
## Visualize Results
"""

# First, some preprocessing to smooth the training and testing arrays for display.
window_length = 100
train_s = np.r_[
    training[window_length - 1 : 0 : -1], training, training[-1:-window_length:-1]
]
test_s = np.r_[
    testing[window_length - 1 : 0 : -1], testing, testing[-1:-window_length:-1]
]
w = np.hamming(window_length)
train_y = np.convolve(w / w.sum(), train_s, mode="valid")
test_y = np.convolve(w / w.sum(), test_s, mode="valid")

# Display the training accuracies.
x = np.arange(0, len(test_y), 1)
plt.plot(x, test_y, x, train_y)
plt.legend(["test", "train"])
plt.grid()

from sklearn.metrics import confusion_matrix, classification_report

print(classification_report(y_test_labels, y_test_preds, target_names=['False', 'True']))

import seaborn as sns
def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True label')
  plt.xlabel('Predicted label');

cm = confusion_matrix(y_test_labels, y_test_preds)
df_cm = pd.DataFrame(cm, index=['False', 'True'], columns=['False', 'True'])
show_confusion_matrix(df_cm)

